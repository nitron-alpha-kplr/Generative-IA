{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CWRw8-Jnp0E"
      },
      "source": [
        "Les Trois Types d'Attention et l'Attention par Produit Scalaire : Cahier de Laboratoire Non Évalué\n",
        "\n",
        "Dans ce Notebook , vous explorerez les trois types d'attention (attention encodeur-décodeur, attention causale et attention self-attention bidirectionnelle) et comment implémenter les deux derniers avec l'attention par produit scalaire.\n",
        "\n",
        "<img src=\"https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Attention%20Models/Week%202/attention_lnb_figs/C4_W2_L3_dot-product-attention_S01_introducing-attention_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "Cette semaine, vous découvrirez comment intégrer l'attention dans les **transformers**. Parce que les transformers ne sont pas des modèles de séquence, ils sont beaucoup plus faciles à paralléliser et à accélérer. En plus de la traduction automatique, les applications des transformers comprennent :\n",
        "* L'auto-complétion\n",
        "* La reconnaissance d'entités nommées\n",
        "* Les chatbots\n",
        "* Les questions-réponses\n",
        "* Et bien plus encore !\n",
        "\n",
        "En plus de l'intégration, du codage de position, des couches denses et des connexions résiduelles, l'attention est un composant essentiel des transformers. Au cœur de tout schéma d'attention utilisé dans un transformer se trouve l'**attention par produit scalaire**, dont les figures ci-dessous présentent une image simplifiée :\n",
        "\n",
        "<img src=\"https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Attention%20Models/Week%202/attention_lnb_figs/C4_W2_L3_dot-product-attention_S03_concept-of-attention_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "<img src=\"https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Attention%20Models/Week%202/attention_lnb_figs/C4_W2_L3_dot-product-attention_S04_attention-math_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "Avec l'attention de base par produit scalaire, vous capturez les interactions entre chaque mot (incorporation) de votre requête et chaque mot de votre clé. Si les requêtes et les clés appartiennent aux mêmes phrases, cela constitue une **auto-attention bidirectionnelle**. Cependant, dans certaines situations, il est plus approprié de considérer uniquement les mots qui précèdent le mot actuel. De tels cas, en particulier lorsque les requêtes et les clés proviennent des mêmes phrases, entrent dans la catégorie de l'**attention causale**.\n",
        "\n",
        "<img src=\"https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Attention%20Models/Week%202/attention_lnb_figs/C4_W2_L4_causal-attention_S02_causal-attention_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "Pour l'attention causale, nous ajoutons un **masque** à l'argument de notre fonction softmax, comme illustré ci-dessous :\n",
        "\n",
        "<img src=\"https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Attention%20Models/Week%202/attention_lnb_figs/C4_W2_L4_causal-attention_S03_causal-attention-math_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "<img src=\"https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Attention%20Models/Week%202/attention_lnb_figs/C4_W2_L4_causal-attention_S04_causal-attention-math-2_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "Maintenant, voyons comment implémenter l'attention avec NumPy. Lorsque vous intégrez l'attention dans un réseau transformer défini avec Trax, vous devrez utiliser `trax.fastmath.numpy` à la place, car les tableaux de Trax sont basés sur les DeviceArrays de JAX. Heureusement, les interfaces des fonctions sont souvent identiques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G98BrXVnp0I"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiRGjAdXnp0I"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import scipy.special\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "\n",
        "# to print the entire np array\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izzh75pdnp0J"
      },
      "source": [
        "Voici quelques fonctions auxiliaires qui vous aideront à créer des tenseurs et à afficher des informations utiles :\n",
        "\n",
        "* `create_tensor()` crée un tableau numpy à partir d'une liste de listes.\n",
        "* `display_tensor()` affiche la forme et le tenseur réel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoETa6AQnp0K"
      },
      "outputs": [],
      "source": [
        "def create_tensor(t):\n",
        "    \"\"\"Create tensor from list of lists\"\"\"\n",
        "    return np.array(t)\n",
        "\n",
        "\n",
        "def display_tensor(t, name):\n",
        "    \"\"\"Display shape and tensor\"\"\"\n",
        "    print(f'{name} shape: {t.shape}\\n')\n",
        "    print(f'{t}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mPYYyJpnp0K"
      },
      "source": [
        "Créez quelques tenseurs et affichez leurs formes. N'hésitez pas à expérimenter avec vos propres tenseurs. Gardez à l'esprit, cependant, que les tableaux de requêtes (query), de clés (key) et de valeurs (value) doivent tous avoir les mêmes dimensions d'intégration (nombre de colonnes), et le tableau de masque (mask) doit avoir la même forme que `np.dot(query, key.T)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4A8-uTPnp0K",
        "outputId": "153fd940-4f69-432b-be32-5451525c216a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "query shape: (2, 3)\n",
            "\n",
            "[[1 0 0]\n",
            " [0 1 0]]\n",
            "\n",
            "key shape: (2, 3)\n",
            "\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "\n",
            "value shape: (2, 3)\n",
            "\n",
            "[[0 1 0]\n",
            " [1 0 1]]\n",
            "\n",
            "mask shape: (2, 2)\n",
            "\n",
            "[[ 0.e+00  0.e+00]\n",
            " [-1.e+09  0.e+00]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
        "display_tensor(q, 'query')\n",
        "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
        "display_tensor(k, 'key')\n",
        "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
        "display_tensor(v, 'value')\n",
        "m = create_tensor([[0, 0], [-1e9, 0]])\n",
        "display_tensor(m, 'mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz9Z4owQnp0L"
      },
      "source": [
        "## Attention à produit scalaire\n",
        "\n",
        "Nous arrivons maintenant à l'essentiel de ce laboratoire, où nous calculons $\\textrm{softmax} \\left(\\frac{Q K^T}{\\sqrt{d}} + M \\right) V$, où le facteur d'échelle (optionnel mais par défaut) $\\sqrt{d}$ est la racine carrée de la dimension d'intégration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0kVUyDfnp0M"
      },
      "outputs": [],
      "source": [
        "def DotProductAttention(query, key, value, mask, scale=True):\n",
        "    \"\"\"Dot product self-attention.\n",
        "    Args:\n",
        "        query (numpy.ndarray): array of query representations with shape (L_q by d)\n",
        "        key (numpy.ndarray): array of key representations with shape (L_k by d)\n",
        "        value (numpy.ndarray): array of value representations with shape (L_k by d) where L_v = L_k\n",
        "        mask (numpy.ndarray): attention-mask, gates attention with shape (L_q by L_k)\n",
        "        scale (bool): whether to scale the dot product of the query and transposed key\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
        "    \"\"\"\n",
        "\n",
        "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
        "\n",
        "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
        "    if scale:\n",
        "        depth = query.shape[-1]\n",
        "    else:\n",
        "        depth = 1\n",
        "\n",
        "    # Calculate scaled query key dot product according to formula above\n",
        "    dots = np.matmul(query, np.swapaxes(key, -1, -2)) / np.sqrt(depth)\n",
        "\n",
        "    # Apply the mask\n",
        "    if mask is not None:\n",
        "        dots = np.where(mask, dots, np.full_like(dots, -1e9))\n",
        "\n",
        "    # Softmax formula implementation\n",
        "    # Use scipy.special.logsumexp of masked_qkT to avoid underflow by division by large numbers\n",
        "    # Note: softmax = e^(dots - logaddexp(dots)) = E^dots / sumexp(dots)\n",
        "    logsumexp = scipy.special.logsumexp(dots, axis=-1, keepdims=True)\n",
        "\n",
        "    # Take exponential of dots minus logsumexp to get softmax\n",
        "    # Use np.exp()\n",
        "    dots = np.exp(dots - logsumexp)\n",
        "\n",
        "    # Multiply dots by value to get self-attention\n",
        "    # Use np.matmul()\n",
        "    attention = np.matmul(dots, value)\n",
        "\n",
        "    return attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo7aqnEgnp0M"
      },
      "source": [
        "Now let's implement the *masked* dot product self-attention (at the heart of causal attention) as a special case of dot product attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y0dPUNonp0N"
      },
      "outputs": [],
      "source": [
        "def dot_product_self_attention(q, k, v, scale=True):\n",
        "    \"\"\" Masked dot product self attention.\n",
        "    Args:\n",
        "        q (numpy.ndarray): queries.\n",
        "        k (numpy.ndarray): keys.\n",
        "        v (numpy.ndarray): values.\n",
        "    Returns:\n",
        "        numpy.ndarray: masked dot product self attention tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Size of the penultimate dimension of the query\n",
        "    mask_size = q.shape[-2]\n",
        "\n",
        "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
        "    # Use np.tril() - Lower triangle of an array and np.ones()\n",
        "    mask = np.tril(np.ones((1, mask_size, mask_size), dtype=np.bool_), k=0)\n",
        "\n",
        "    return DotProductAttention(q, k, v, mask, scale=scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEng6w7Znp0N"
      },
      "outputs": [],
      "source": [
        "dot_product_self_attention(q, k, v)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}